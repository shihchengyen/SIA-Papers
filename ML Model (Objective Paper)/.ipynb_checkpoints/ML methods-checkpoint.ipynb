{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345315ff-ad0d-4df9-b7e4-9863d03d538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, BayesianRidge, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, StackingClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler, NeighbourhoodCleaningRule, AllKNN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66119f4a-581b-47fe-bc57-bc68504df736",
   "metadata": {},
   "source": [
    "## Cross validation subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eaaae0c-3b52-4850-a0e8-f7bdc359599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unchanged = pd.read_csv('ML_model_data.csv')\n",
    "df = df_unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6d7898-fa68-4872-b807-cb2cbf145425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 152.0\n"
     ]
    }
   ],
   "source": [
    "df_cv_split = df_unchanged[df_unchanged['top_152']].drop_duplicates() ## Keep 152 positive and significant subjects\n",
    "# df_cv_split = df_unchanged.drop_duplicates() ## Uncomment to take all 208 subjects\n",
    "\n",
    "# Optional cross validation subjects -> They will not be used in training-testing\n",
    "all_cv_subjects = []#'A020', 'A024', 'A040', 'A041', 'A048', 'A098', 'A105', 'A121','A141', 'A168', 'A173', 'A180', 'A198', 'A210']\n",
    "best_cv_subjects = list(df_cv_split[df_cv_split['subject'].isin(all_cv_subjects)]['subject'])\n",
    "best_other_subjects = list(df_cv_split[~df_cv_split['subject'].isin(all_cv_subjects)]['subject'])\n",
    "\n",
    "df_cv = df_unchanged[df_unchanged['subject'].isin(best_cv_subjects)]\n",
    "df = df_unchanged[df_unchanged['subject'].isin(best_other_subjects)]\n",
    "print(f'{df_cv.shape[0] / 19} {df.shape[0] / 19}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17b573-8c66-446a-80d6-eedcccdc5cda",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae3185-41a4-454d-ae1c-a48ddbc12f38",
   "metadata": {},
   "source": [
    "#### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd71c11-d60b-4fb3-9333-244b25032554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subjects(ml_df, test_size):\n",
    "    subjects = ml_df.subject.unique()\n",
    "    subjects_train, subjects_test = train_test_split(subjects, test_size=test_size)\n",
    "\n",
    "    return subjects_train, subjects_test\n",
    "\n",
    "def get_x_y(subjects, ml_df, rating_col, num_vars):\n",
    "    df = ml_df[ml_df.subject.isin(subjects)]\n",
    "    \n",
    "    X = df.drop(columns=['subject', rating_col, 'slope'])\n",
    "    y = df[rating_col]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X\n",
    "    X_scaled[num_vars] = scaler.fit_transform(X[num_vars])\n",
    "    \n",
    "    return X_scaled, y\n",
    "    \n",
    "def y_cat(df, rating_col, cats, model_name=False):    \n",
    "    if rating_col == 'total_rating' and cats == 2:\n",
    "        df = df.apply(lambda x: 'Low' if x <= 39 else 'High')\n",
    "        \n",
    "    elif rating_col == 'total_rating' and cats == 3:\n",
    "        df = df.apply(lambda x: 'Low' if x <= 30 else ('Medium' if x <= 40 else 'High'))\n",
    "        \n",
    "    elif rating_col == 'value' and cats == 2:\n",
    "        df = df.apply(lambda x: 'Low' if x <= 0.5 else 'High')\n",
    "        \n",
    "    elif rating_col == 'value' and cats == 3:\n",
    "        df = df.apply(lambda x: 'Low' if x <= 0.334 else ('Medium' if x <= 0.667 else 'High'))\n",
    "        \n",
    "    else:\n",
    "        df = df\n",
    "\n",
    "    if model_name in ['xgb', 'ensemble']:\n",
    "        if cats == 2:\n",
    "            df = df.apply(lambda x: 0 if x == 'Low' else 1)\n",
    "        elif cats == 3:\n",
    "            df = df.apply(lambda x: 0 if x == 'Low' else 1 if x == 'Medium' else 2)\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d698af3-650b-465d-9fcc-75d4e3a28b1c",
   "metadata": {},
   "source": [
    "#### Create DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486251f0-3148-4700-bc2b-6bc219075909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mldf(df, wsizes, features, rating_col, is_time_step):\n",
    "    icm_features = [f'{feature}_{ws}'  for ws in wsizes for feature in features]\n",
    "    ml_df = df.copy().dropna(subset=[rating_col])\n",
    "\n",
    "    num_vars = icm_features\n",
    "    if not is_time_step:\n",
    "        num_vars = (num_vars + ['time_step'])\n",
    "\n",
    "    ml_df = ml_df[['subject', 'slope', rating_col]+num_vars].dropna(subset=num_vars)\n",
    "    print(f'Count before: {len(df) / 19} Count after: {len(ml_df) / 19}')\n",
    "    return num_vars, ml_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55274db1-f713-4c29-a15a-7f4edb8c4729",
   "metadata": {},
   "source": [
    "#### Classification ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be628bb6-fd1c-40b0-bc8c-fc2afc970e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_ml_model_class(model_name, ml_df, ml_df_cv, labels, rating_col, num_vars, n_classes, repeat):\n",
    "    def sample(sampling_method, X, y):\n",
    "        if sampling_method == 'random_oversample':\n",
    "            sampler = RandomOverSampler(sampling_strategy='minority')\n",
    "        elif sampling_method == 'smote':\n",
    "            sampler = SMOTE(k_neighbors=1, sampling_strategy='minority')\n",
    "        elif sampling_method == 'smoteenn': \n",
    "            sampler = SMOTEENN(sampling_strategy=0.5)\n",
    "        elif sampling_method == 'random_undersample':\n",
    "            sampler = RandomUnderSampler(sampling_strategy=0.8) # 0.5\n",
    "        elif sampling_method == 'smotetomek':\n",
    "            sampler = SMOTETomek()\n",
    "        elif sampling_method == 'ncr':\n",
    "            sampler = NeighbourhoodCleaningRule()\n",
    "        else:\n",
    "            print('Not a valid sampling method')\n",
    "            return X, y\n",
    "\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "        return X_resampled, y_resampled\n",
    "        \n",
    "    def execute_ml_model_class(model_name, X_train_scaled, y_train, i, verbose=True):    \n",
    "        seed = i + 42\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        if model_name == 'rfc':\n",
    "            model = BalancedRandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=3, max_features='sqrt', random_state=seed)\n",
    "        elif model_name == 'lr':\n",
    "            model = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "        elif model_name == 'xgb':\n",
    "            ratio = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "            model = XGBClassifier(scale_pos_weight=ratio, use_label_encoder=False, eval_metric='logloss', random_state=seed)\n",
    "        elif model_name == 'ensemble':\n",
    "            xgb_model = XGBClassifier(n_estimators=100, random_state=seed)\n",
    "            lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=seed)\n",
    "            rf_model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "            svm_model = SVC(probability=True, random_state=seed)\n",
    "            knn_model = KNeighborsClassifier()\n",
    "            lr_model = LogisticRegression()\n",
    "            \n",
    "            meta_model = LogisticRegression()\n",
    "            \n",
    "            model = StackingClassifier(\n",
    "                estimators=[('rf', rf_model)],\n",
    "                final_estimator=meta_model,\n",
    "                random_state=seed\n",
    "            )\n",
    "    \n",
    "        model.fit(X_train_scaled, y_train)            \n",
    "        y_pred = model.predict(X_train_scaled)\n",
    "\n",
    "        report = classification_report(y_train, y_pred, output_dict=True)\n",
    "    \n",
    "        feature_importance_df = None\n",
    "        if model_name in ['rfc'] and verbose:\n",
    "            feature_importance_df = pd.DataFrame({'Feature': X_train_scaled.columns, 'Importance': model.feature_importances_})\n",
    "    \n",
    "        return model, report, feature_importance_df\n",
    "    \n",
    "    def execute_cv(model, X, y):\n",
    "        y_pred = model.predict(X)\n",
    "        cv_report = classification_report(y, y_pred, output_dict=True)\n",
    "        roc_auc = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "\n",
    "        # display(confusion_matrix(y, y_pred))\n",
    "    \n",
    "        return cv_report, roc_auc\n",
    "        \n",
    "    def initialise_metrics(labels):\n",
    "        labels = labels + ['macro avg', 'weighted avg']\n",
    "        metric_dict = {'precision': {label: [] for label in labels}, 'recall': {label: [] for label in labels}, \n",
    "                       'f1_score': {label: [] for label in labels}, 'accuracy':[], 'feature_importance': []}\n",
    "        \n",
    "        for label in labels:\n",
    "            metric_dict[f'n_train_{label}'] = []\n",
    "            metric_dict[f'n_test_{label}'] = []\n",
    "            metric_dict[f'n_cv_{label}'] = []\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    def update_metric_dict(report, feature_importance_df, result_dict):\n",
    "        for label in labels + ['macro avg', 'weighted avg']:\n",
    "            precision = report[str(label)]['precision']\n",
    "            recall = report[str(label)]['recall']\n",
    "            f1 = report[str(label)]['f1-score']\n",
    "            \n",
    "            result_dict['precision'][label].append(precision)\n",
    "            result_dict['recall'][label].append(recall)\n",
    "            result_dict['f1_score'][label].append(f1)\n",
    "            \n",
    "        result_dict['accuracy'] = report['accuracy']\n",
    "        result_dict['feature_importance'].append(feature_importance_df)\n",
    "\n",
    "\n",
    "    def print_metrics(result_dict, labels):\n",
    "        report_data = []\n",
    "        for label in labels + ['macro avg', 'weighted avg']:\n",
    "            report_data.append([label, \n",
    "                                f\"{np.mean(result_dict['precision'][label]):.3f}\",\n",
    "                                f\"{np.mean(result_dict['recall'][label]):.3f}\",\n",
    "                                f\"{np.mean(result_dict['f1_score'][label]):.3f}\"])\n",
    "        report_data.append(['accuracy', \"\", \"\", np.mean(result_dict['accuracy'])])\n",
    "    \n",
    "        print(pd.DataFrame(report_data, columns=['Label', 'Precision', 'Recall', 'F1-score']).to_string(index=False))\n",
    "\n",
    "    def track_y_label(result_dict, labels, y_train, y_test, y_cv):\n",
    "        for label in labels:\n",
    "            result_dict[f'n_train_{label}'].append(y_train[y_train == label].shape[0])\n",
    "            result_dict[f'n_test_{label}'].append(y_test[y_test == label].shape[0])\n",
    "            if y_cv != None:\n",
    "                result_dict[f'n_cv_{label}'].append(y_cv[y_cv == label].shape[0])\n",
    "\n",
    "    train_dict = initialise_metrics(labels)\n",
    "    result_dict = initialise_metrics(labels)\n",
    "    cv_result_dict = initialise_metrics(labels)\n",
    "    cv_subject_metrics = {s: initialise_metrics(labels) for s in best_cv_subjects}\n",
    "    cv_aucs = []\n",
    "    \n",
    "    for i in range(repeat):\n",
    "        ### Get df's\n",
    "        # Train-Test\n",
    "        subjects_train, subjects_test = split_subjects(ml_df, 0.2)\n",
    "        X_train_scaled, y_train = get_x_y(subjects_train, ml_df, rating_col, num_vars)\n",
    "        X_test_scaled, y_test = get_x_y(subjects_test, ml_df, rating_col, num_vars)\n",
    "        y_train, y_test = y_cat(y_train, rating_col, n_classes, model_name), y_cat(y_test, rating_col, n_classes, model_name)\n",
    "\n",
    "        # Sampling (Comment out for model without sampling)\n",
    "        sampling_method = ['None', 'random_oversample', 'smote', 'smoteenn', 'random_undersample', 'smotetomek', 'ncr'][4]\n",
    "        if sampling_method != 'None':\n",
    "            X_train_scaled, y_train = sample(sampling_method, X_train_scaled, y_train)\n",
    "\n",
    "        # CV\n",
    "        y_train_cv = None\n",
    "        if len(best_cv_subjects) != 0:\n",
    "            X_train_scaled_cv, y_train_cv = get_x_y(best_cv_subjects, ml_df_cv, rating_col, num_vars)\n",
    "            y_train_cv = y_cat(y_train_cv, rating_col, n_classes, model_name)\n",
    "\n",
    "        # Keep track of y-labels\n",
    "        track_y_label(result_dict, labels, y_train, y_test, y_train_cv)\n",
    "\n",
    "        # print(f'{len(subjects_train)} {len(subjects_test)}')\n",
    "\n",
    "        ### Execute model\n",
    "        # Train\n",
    "        model, train_report, feature_importance_df = execute_ml_model_class(model_name, X_train_scaled, y_train, i, True)\n",
    "        update_metric_dict(train_report, feature_importance_df, train_dict)\n",
    "\n",
    "        # Test\n",
    "        test_report, test_auc = execute_cv(model, X_test_scaled, y_test)\n",
    "        update_metric_dict(test_report, None, result_dict)\n",
    "\n",
    "        # CV\n",
    "        if len(best_cv_subjects) != 0:\n",
    "            cv_report, cv_auc = execute_cv(model, X_train_scaled_cv, y_train_cv)\n",
    "            update_metric_dict(cv_report, None, cv_result_dict)\n",
    "\n",
    "            cv_aucs.append(cv_auc)\n",
    "\n",
    "    metric_data = {'Training': train_dict, 'Testing': result_dict, 'CV': cv_result_dict}\n",
    "    for metric_data_name in metric_data:\n",
    "        print(f\"\\n{metric_data_name} data metrics\")\n",
    "        print_metrics(metric_data[metric_data_name], labels)\n",
    "    \n",
    "    plot_feature_importance(model_name, train_dict['feature_importance'])\n",
    "\n",
    "    for label in labels:\n",
    "        result = ''\n",
    "        for rtype in ['train', 'test', 'cv']:\n",
    "            result = result + f'{label} {rtype}: {np.mean(result_dict[f\"n_{rtype}_{label}\"])} '\n",
    "        print(result)\n",
    "\n",
    "    print(np.mean(cv_aucs))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf2b16-d966-411a-88dc-7ec9df81b1d8",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a34d5b4-7439-4879-860a-23c88bba0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model_name, df_list):\n",
    "    if model_name not in ['rf', 'rfc']:\n",
    "        return\n",
    "        \n",
    "    all_importances = pd.concat(df_list, axis=0)\n",
    "    avg_importance = all_importances.groupby('Feature')['Importance'].mean().reset_index()\n",
    "    feature_importance_df = avg_importance.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "    feature_importance_df['cum_sum'] = feature_importance_df['Importance'].cumsum()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    display(feature_importance_df)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d83da1-d93d-446d-abcc-0d7d69f302aa",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2423290-f063-42be-9d0b-7a522e5b251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose ICM feature\n",
    "features = ['icm_pressdiff_count']#, 'icm_ca_count', 'icm_centroid_count', 'icm_all_count']\n",
    "\n",
    "## Uses ICM to predict the total rating\n",
    "rating_col = 'total_rating'\n",
    "\n",
    "## False -> exclude time & True -> include time\n",
    "no_num_vars = [False, True][0]\n",
    "\n",
    "## All the window sizes to be included (multiple window sizes can be included)\n",
    "wsizes = [1]\n",
    "\n",
    "## model_name: 'rfc' -> random forest classifier & 'lr' -> logistic regression // Other models are available too (see function execute_ml_model_class)\n",
    "## n_classes: number of classes to be classified into (see function y_cat)\n",
    "## repeat: the number of times the model is to be repeated (the final results are the average of the total number of repeats)\n",
    "model_name, n_classes, repeat = 'lr', 2, 50\n",
    "\n",
    "if model_name in ['xgb', 'ensemble']:\n",
    "    labels = [0, 1] if n_classes == 2 else [0, 1, 2]\n",
    "else:\n",
    "    labels = ['Low', 'High'] if n_classes == 2 else ['Low', 'Medium', 'High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b45ce37-6f9c-49e8-b4f3-5b1baf4de004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Testing\n",
      "Count before: 152.0 Count after: 152.0\n",
      "CV\n",
      "Count before: 0.0 Count after: 0.0\n",
      "\n",
      "Training data metrics\n",
      "       Label Precision Recall  F1-score\n",
      "         Low     0.733  0.711     0.721\n",
      "        High     0.652  0.677     0.664\n",
      "   macro avg     0.692  0.694     0.693\n",
      "weighted avg     0.697  0.695     0.696\n",
      "    accuracy                   0.659091\n",
      "\n",
      "Testing data metrics\n",
      "       Label Precision Recall F1-score\n",
      "         Low     0.962  0.706    0.814\n",
      "        High     0.134  0.643    0.216\n",
      "   macro avg     0.548  0.674    0.515\n",
      "weighted avg     0.908  0.700    0.775\n",
      "    accuracy                   0.70798\n",
      "\n",
      "CV data metrics\n",
      "       Label Precision Recall F1-score\n",
      "         Low       nan    nan      nan\n",
      "        High       nan    nan      nan\n",
      "   macro avg       nan    nan      nan\n",
      "weighted avg       nan    nan      nan\n",
      "    accuracy                       NaN\n",
      "Low train: 172.0 Low test: 548.88 Low cv: nan \n",
      "High train: 137.88 High test: 40.12 High cv: nan \n",
      "nan\n",
      "Output col: total_rating; window size: [1]; Repeats: 50;\n",
      "Vars: ['icm_pressdiff_count_1', 'time_step']\n"
     ]
    }
   ],
   "source": [
    "print(\"Training-Testing\")\n",
    "num_vars, ml_df = get_mldf(df, wsizes, features, rating_col, no_num_vars)\n",
    "print(\"CV\")\n",
    "_, ml_df_cv = get_mldf(df_cv, wsizes, features, rating_col, no_num_vars)\n",
    "\n",
    "## Note: Sampling method options can be chosen in function repeat_ml_model_class\n",
    "repeat_ml_model_class(model_name, ml_df, ml_df_cv, labels, rating_col, num_vars, n_classes, repeat)  \n",
    "\n",
    "print(f'Output col: {rating_col}; window size: {wsizes}; Repeats: {repeat};\\nVars: {num_vars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97f0ea-a303-4be5-a3a8-9c7e1cf4e214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
